\documentclass{article}
\usepackage{graphics}
\usepackage{indentfirst}
\usepackage{amsmath}
\author{Ruichen Wang}
\title{Starting from Entropy}

\begin{document}
\maketitle


\begin{abstract}
Starting with the origins of information, extend to entropy and its families. and some introduction and explanation of entropy related algorithms. Like log-likelihood, logistic regression, variational auto encoder(VAE),generative adversarial network(GAN),etc.
\end{abstract}

\tableofcontents
\section{What is information?} 
\subsection{Defination of information}
How to measure the uncentainty of certain event in a mathematics?
\paragraph{Given} x is some event, P(x) is probability which event x happens. Intuitively, the information should have inverse proportion to the probability, which is 
$$I(x)=\frac{1}{P(x)}$$
We also want the information more stable,remove the division, so
$$I(x)=log\frac{1}{P(x)}=-logP(x)$$
\subsection{Property of information}
As a result, the $-logP(x)$ has every properties we want:
\begin{itemize}
\item Lower probability, higher information
\item Higher probability, lower information
\item Multi-event happens, the probability is multiplied. the information is summed
\end{itemize}
\paragraph{Mathematics}
As we know, $P(x) \in [0,1]$, and larger P(x) should have smaller information.
$$P(x_{1},x_{2})=P(x_{1})*P(x_{2})$$
$$logP(x_{1},x_{2})=logP(x_{1})+logP(x_{2})$$
\section{Entorpy (Expectation(sum) of Information)}
\subsection{Shannon`s Information Theory}
Claude Elwood Shannon(1916-2001).\\
1937 MIT Master degree.\\
1940 MIT Ph.D degree from MIT. \\
1948 Published a landmark paper 'A mathematical Theory of Communication'.\\
Entropy is defined as:
$$H(x)=\sum_{i=1}^{n}P(x_{i})I(x_{i})=-\sum_{i=1}^{n}P(x_{i})logP(x_{i})$$
\subsection{Property of entropy}
The property of the entropy is quite simple
\begin{itemize}
\item Higher probability, the less information, the lower entropy
\item Non-negative, every event has some information
\item Cumulative, multile events happens, the information is the sum of them.
\end{itemize}
\section{Families of entropy}
\subsection{Cross-Entropy}
Suppose we don`t know P(x) yet, so we make an 'artificial' probability distribution Q(x). How can we measure the cost as we using Q(x) to approximate P(x)? We define corss entropy as:
$$H(P,Q)=-\sum_{x}P(x)logQ(x)$$

\paragraph{Estimation} There are many situations where P(x) is unknown. Given a test set N observed, which comes from a Monte Carlo sampling of the true distribution P(x). Cross entropy is calculated using :
$$H(T,Q)=-\sum_{i=1}^{N}\frac{1}{N}logQ(x_{i})$$
\subsubsection{Relation to log-likelihood}
for the maximum likelihood estimation (MAE), we have:
$$\prod_{i}q_{i}^{N_{p_{i}}}$$
So log-likelihood,divided by N is :
$$\frac{1}{N}log\prod_{i}q_{i}^{N_{p_{i}}}=\sum_{i}p_{i}logq_{i}=-H(p,q)$$
So maximum the likelihood is the same as minimizing the cross entropy.
\subsubsection{Cross-entropy loss in classification}
In machine learning, cross-entropy loss is widely used now, it often defines as : 
$$L=-ylog(y^{'})=H(y,y^{'})$$
It describes the distance between the prediction and truth.
\subsubsection{The simple and elegant relationship with softmax}
This is worth talking here. As the softmax probability and cross-entropy loss is so so common, and they often work together.\\
Softmax function:
$$p_{i}=\frac{e^{a_{i}}}{\sum_{k=1}^{N}e^{a_{k}}}$$
Derivative of softmax $\frac{\partial p_{i}}{\partial \alpha_{j}}$: \\
\begin{equation}
\frac{\partial p_{i}}{\partial \alpha_{j}}=\left\{
\begin{aligned}
p_{i}(1-p_{j}) & & i=j \\
-p_{j}*p_{i}  && i\neq j \\
\end{aligned}
\right.
\end{equation}
The cross entropy loss:
$$L=-\sum_{i}y_{i}logp_{i}$$
Derivative of cross entropy loss:
$$\frac{\partial L}{\partial o_{i}}=-\sum y_{k}\frac{1}{p_{k}}*\frac{\partial p_{i}}{\partial \alpha_{j}}$$
From the dervative of softmax we derived earlier,
$$\frac{\partial L}{\partial o_{i}}=-y_{i}(1-p_{i})-\sum_{k \neq i}y_{k}\frac{1}{p_{k}}(-p_{k}*p_{i})$$
$$=p_{i}(y_{i}+\sum _{k \neq i}y_{k})-y_{i}=p_{i}-y_{i}$$
This is why we often use softmax and cross entropy together, The gradient is quite simple to calculate. 
\subsection{Kullback-Leibler divergence}
KL divergence is also called relative entropy. It is a measure of how one probability distribution is different from a second.\\
For discrete probability distirbutions P and Q defined on the same probability space, the KL divergence from Q to P (Q with respect to P) is defined as :
$$D_{KL}(P \parallel Q)=-\sum_{i}P(i)log(\frac{Q(i)}{P(i)})$$
Actually, this can also be written as: 
$$D_{KL}(P \parallel Q)=H(P,Q)-H(P)$$
Which means the more entropy using Q gets with respect to original distirbution P.
\subsubsection{Interpretations}
In machine learning, $D_{KL}(P \parallel Q)$ is often called the information gain achieved if Q is used instead of P.
\\
Expressed in the language of Bayesian inference, $D_{KL}(P \parallel Q)$ is a measure of the information gained when one revises one's beliefs from the prior probability distribution Q to the posterior probability distribution P.\\
In applications, P typically represents the true distribution of data. Q represents the model. Minimize $D_{KL}(P \parallel Q)$ is to find a Q that closest to P.  
\subsubsection{Property of KL}
\begin{itemize}
\item Non-negative \\
As a result known as Gibbs`s inequality, with $D_{KL}(P \parallel Q)$ zero if and only if P = Q.
\item Asymmetric
$$D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)$$
we can define symmetrised divergence as:
$$\frac{D_{KL}(P \parallel Q) + D_{KL}(Q \parallel P)}{2}$$
\end{itemize}

\subsubsection{Applications}
Generative models like VAE, we may need a new section to go through this.I will go into details later. Here I just put a bayes equation here :). 
$$posterior=\frac{likelihood * prior}{evidence}$$
\section{Variational Bayesian Inference}
\subsection{Variational inference}
\paragraph{Question description} Suppose we have observations x, and hidden variables z, and some fixed parameters $\alpha$.What we want is the posterior distribution.
$$p(z|x,\alpha)=\frac{p(z,x|\alpha)}{\int_z p(z,x|\alpha)dz}$$
In many cases, the $\int_z p(z,x|\alpha)dz$ is intractable. we don`t know how to compute it especially in high dimensions.
\paragraph{Solution} The main idea behind variational methods is to pick a family of distributions over the latent variables with its own \textbf{variational parameters}.
$$q(z_{1:m}|v)$$
Then find $v$ to make $q$ close to the posterior.
\subsection{KL divergence measure}
As mentioned above, we can use KL for this variational inference:
$$D_{KL}(q \parallel p)=E_{q} \left[ log \frac{q(Z)}{p(Z|x)} \right]$$
\end{document}


























