\documentclass{article}
\usepackage{graphics}
\usepackage{indentfirst}
\usepackage{amsmath}
\author{Ruichen Wang}
\title{Starting from Information}

\begin{document}
\maketitle


\begin{abstract}
Starting with the introducing the origins of information, extend to entropy and its families. and some introduction and explanation of entropy related algorithms. Like log-likelihood, logistic regression,variational bayesian inference, variational auto encoder(VAE),generative adversarial network(GAN),etc.
\end{abstract}

\tableofcontents
\section{What is Information?} 
\subsection{Defination of Information}
How to measure the uncentainty of certain event in a mathematics?
\paragraph{Given} x is some event, P(x) is probability which event x happens. Intuitively, the information should have inverse proportion to the probability, which is 
$$I(x)=\frac{1}{P(x)}$$
We also want the information more stable,remove the division, so
$$I(x)=log\frac{1}{P(x)}=-logP(x)$$
\subsection{Property of Information}
As a result, the $-logP(x)$ has every properties we want:
\begin{itemize}
\item Lower probability, higher information
\item Higher probability, lower information
\item Multi-event happens, the probability is multiplied. the information is summed
\end{itemize}
\paragraph{Mathematics}
As we know, $P(x) \in [0,1]$, and larger P(x) should have smaller information.
$$P(x_{1},x_{2})=P(x_{1})*P(x_{2})$$
$$logP(x_{1},x_{2})=logP(x_{1})+logP(x_{2})$$
\section{Entorpy (Expectation(sum) of Information)}
\subsection{Shannon's Information Theory}
Claude Elwood Shannon(1916-2001).\\
1937 MIT Master degree.\\
1940 MIT Ph.D degree from MIT. \\
1948 Published a landmark paper 'A mathematical Theory of Communication'.\\
Entropy is defined as:
$$H(x)=E[I(x)]=\sum_{i=1}^{n}P(x_{i})I(x_{i})=-\sum_{i=1}^{n}P(x_{i})logP(x_{i})$$
\subsection{Property of Entropy}
The property of the entropy is quite simple
\begin{itemize}
\item Higher probability, the less information, the lower entropy
\item Non-negative, every event has some information
\item Cumulative, multile events happens, the information is the sum of them.
\end{itemize}
\section{Families of Entropy}
\subsection{Cross-Entropy}
Suppose we don't know P(x) yet, so we make an 'artificial' probability distribution Q(x). How can we measure the cost as we using Q(x) to approximate P(x)? We define corss entropy as:
$$H(P,Q)=-\sum_{x}P(x)logQ(x)$$

\paragraph{Estimation} There are many situations where P(x) is unknown. Given a test set N observed, which comes from a Monte Carlo sampling of the true distribution P(x). Cross entropy is calculated using :
$$H(T,Q)=-\sum_{i=1}^{N}\frac{1}{N}logQ(x_{i})$$
\subsubsection{Relation to Log-likelihood}
for the maximum likelihood estimation (MAE), we have:
$$\prod_{i}q_{i}^{N_{p_{i}}}$$ 
So log-likelihood,divided by N is :
$$\frac{1}{N}log\prod_{i}q_{i}^{N_{p_{i}}}=\sum_{i}p_{i}logq_{i}=-H(p,q)$$
So maximum the likelihood is the same as minimizing the cross entropy.
\subsubsection{Cross-entropy Loss in Classification}
In machine learning, cross-entropy loss is widely used now, it often defines as : 
$$L=-ylog(y^{'})=H(y,y^{'})$$

It describes the distance between the prediction and truth.
\subsubsection{Simple and Elegant Relationship with Softmax}
As the softmax probability and cross-entropy loss is so so common, and they often work together. But why? Because the simplicity of the derivative. \\
Softmax function:
$$p_{i}=\frac{e^{a_{i}}}{\sum_{k=1}^{N}e^{a_{k}}}$$
Derivative of softmax $\frac{\partial p_{i}}{\partial \alpha_{j}}$: \\
\begin{equation}
\nonumber
\frac{\partial p_{i}}{\partial \alpha_{j}}=\left\{
\begin{aligned}
p_{i}(1-p_{j}) & & i=j \\
-p_{j}*p_{i}  && i\neq j \\
\end{aligned}
\right.
\end{equation}
The cross entropy loss:
$$L=-\sum_{i}y_{i}logp_{i}$$
Derivative of cross entropy loss:
$$\frac{\partial L}{\partial o_{i}}=-\sum y_{k}\frac{1}{p_{k}}*\frac{\partial p_{k}}{\partial o_{i}}$$
From the dervative of softmax we derived earlier,
$$\frac{\partial L}{\partial o_{i}}=-y_{i}(1-p_{i})-\sum_{k \neq i}y_{k}\frac{1}{p_{k}}(-p_{k}*p_{i})$$
$$=p_{i}(y_{i}+\sum _{k \neq i}y_{k})-y_{i}=p_{i}-y_{i}$$
This is why we often use softmax and cross entropy together, The gradient is quite simple to calculate. 
\subsection{Kullback-Leibler Divergence}
KL divergence is also called relative entropy. It is a measure of how one probability distribution is different from a second.\\
For discrete probability distirbutions P and Q defined on the same probability space, the KL divergence from Q to P (Q with respect to P) is defined as :
$$D_{KL}(P \parallel Q)=-\sum_{i}P(i)log(\frac{Q(i)}{P(i)})$$
This can also be written as the cross entropy minius original entropy: 
$$D_{KL}(P \parallel Q)=H(P,Q)-H(P)$$
Which means the more entropy using Q generates with respect to original distirbution P.
\subsubsection{Interpretations}
In machine learning, $D_{KL}(P \parallel Q)$ is often called the information gain achieved if Q is used instead of P.


Expressed in the language of Bayesian inference, $D_{KL}(P \parallel Q)$ is a measure of the information gained when one revises one's beliefs from the prior probability distribution Q to the posterior probability distribution P.

In applications, P typically represents the true distribution of data. Q represents the model. Minimize $D_{KL}(P \parallel Q)$ is to find a Q that closest to P.  
\subsubsection{Property of KL}
\begin{itemize}
\item Non-negative \\
As a result known as Gibbs's inequality, with $D_{KL}(P \parallel Q)$ zero if and only if P = Q.
\item Asymmetric
$$D_{KL}(P \parallel Q) \neq D_{KL}(Q \parallel P)$$
we can define symmetrised divergence as:
$$\frac{D_{KL}(P \parallel Q) + D_{KL}(Q \parallel P)}{2}$$
\end{itemize}
\subsubsection{Applications}
Generative models like VAE, we may need a new section to go through this.I will go into details later. Here I just put a bayesian equation here :). 
$$posterior=\frac{likelihood * prior}{evidence}$$
$$p(z|x)=\frac{p(x|z)p(z)}{p(x)}$$

\section{Variational Bayesian Inference}
\subsection{Variational Inference}
The idea behind variational inference is to first posit a family of densities and then
to find the member of that family which is close to the target. Closeness is measured
by Kullback-Leibler divergence.

Variational inference is widely used to approximate posterior densities for Bayesian models,an alternative strategy to Markov chain Monte Carlo (MCMC) sampling. Compared to MCMC, MCMC algorithms sample a Markov chain, variational algorithms solve an
optimization problem. variational inference tends to be faster and easier to scale to large data.

Variational inference has a close relationship with EM algorithm. You can view VAE, GAN as a certain form of variational inference. Variational inference doesn't have the global optimal point, that why VAE and GAN are very hard to train or converge.
\paragraph{*Tips} Compare MCMC with Metropolis-Hasting (MH). MH larger the acceptance ratio $\alpha$. When extend to high dimensions, It is called Gibbs sampling. Actually, they are based on the same idea - bayesian stationary distribution. 

\paragraph{Question description} Suppose we have observations x, and hidden variables z, and some fixed parameters $\alpha$.What we want is the posterior distribution.
$$p(z|x,\alpha)=\frac{p(z,x|\alpha)}{\int_z p(z,x|\alpha)dz}$$
In many cases, the $\int_z p(z,x|\alpha)dz$ is intractable. we don't know how to compute it especially in high dimensions.
\paragraph{Solution} The main idea behind variational methods is to pick a family of distributions over the latent variables with its own \textbf{variational parameters}.
$$q(z_{1:m}|v)$$
Then find $v$ to make $q$ close to the posterior.
\subsection{KL Divergence Measure}
As mentioned above, we can use KL for this variational inference:
$$D_{KL}(q(z) \parallel p(z|x))=E_{q(z)} \left[ log \frac{q(z)}{p(z|x)} \right]$$
Intuitively, According to this formula, there are three cases:
\begin{itemize}
\item If q is low, then we don't care (Because of the expectation)
\item If q is high and p is high, good :)
\item If q is high and p is low, bad :(
\end{itemize}
\subsection{Evidence Lower Bound}
Actually we can not minimize KL divergence. But we can minimize another function which is equal to this. This is evidence lower bound (ELBO).
\subsubsection{Jensen's Inequality}
Jensen's inequality are widely used in EM algorithm. In convex function, we have :
$$f(tx_{1}+(1-t)x_{2})\leq tf(x_{1})+(1-t)f(x_{2}) $$ 
In the context of probability theory, if X is a random variable, and $\varphi$ is a convex function, then:
$$\varphi(E[x]) \leq E[\varphi(x)]$$
\paragraph{Back} to the problem, we have observations $x^{1},x^{2},...,x^{n}$, we want $p(x^{i})$ get the max probability. Using MLE on it, which is the sum of the log-likelihood,
$$logp_{\theta}(x^{1},x^{2},...,x^{n})=\sum_{i=1}^{N}logp_{\theta} (x^{i})$$
and

\begin{align*}
logp(x) & =log \int_{z}p(x,z) \\
 &= log \int_{z}p(x,z)\frac{q(z)}{q(z)} \\
 &= log \left( E_{q} \left[ \frac{p(x,z)}{q(z)} \right] \right) \\
 &\geq E_{q} \left[ log \frac{p(x,z)}{q(z)} \right] \\
 &\geq E_{q}[logp(x,z)]-E_{q}[logq(z)]
\end{align*}
Note the second term is the entropy\\
But what does this have to do with the KL divergence? 
\subsubsection{KL Transformation}
As mentioned, we want $q(z)$ and $q(z|x)$ are close to each other:
\begin{align*}
KL(q(z)||p(z|x)) &= E_{q}\left[ log \frac{q(z)}{p(z|x)} \right] \\
&=E_{q}[logq(z)]-E_{q}[logp(z|x)]
\end{align*}
As we know,
$$p(z|x)=\frac{p(z,x)}{p(x)}$$
so 
\begin{align*}
KL(q(z)||p(z|x)) &= E_{q}[logq(z)]-E_{q}[logp(z,x)]+E_{q}[logp(x)] \\
&= -(E_{q}[logp(z,x)]-E_{q}[logq(z)])+logp(x)
\end{align*}
The first term is ELBO we just met.\\
The formula can also be written as :
$$logp(x)=KL(q(z)||p(z|x))+(E_{q}[logp(z,x)]-E_{q}[logq(z)])$$

As I mentioned before, For two different distributions, KL divergence is always non-negative. and $p(x)$ is the observation evidence, which is fixed. So minimizing the KL divergence is the same as maximizing the ELBO. This is also called as the variational lower bound.
\subsubsection{Relationship with EM}
EM algorithm is also known as a famous method to find the distirbutions of latent varibales. Unlike variational inference we are going to talk about, EM algorithm use the fact that ELBO is equal to the $p(x)$ when $q(z)=p(z|x)$. EM \textbf{alternates} between computing $p(z|x)$ (E step), and optimizing it with respect to the model parameters(M step). The biggest difference is EM assume $p(z|x)$ is computable and fix the parameter, use it, while variational inference use bayesian setting and apply to the models we can not compute. 

* EM is out the scope of this article, I don't want to go into too detail about it. Actually the formula below explains pretty clear.\\
E-step:
$$q(z):=p(z|x;\theta)$$
M-step:
$$\theta:=\mathop{\arg\max}_{\theta} -KL(q(z)||p(z|x;\theta))$$


\subsection{Mean Field Theory}
Mean field theory is also called \textbf{self-consistent field theory}. It studies the behavior of large and complex stochastic models by studying a simpler model.Such models consider a large number of small individual components that interact with each other.

The effect of all the other individuals on any given individual is approximated by a single averaged effect, thus reducing a many-body problem to a one-body problem.
\subsubsection{Mean Field Approximation}
We assume each variable is independent.Using this theory, we can write:
$$q(z_{1:m})= \prod _{i=1}^{m}q(z_{i})$$
$$E_{q}[logq(z_{1:m})]=\sum_{j=1}^{m}E_{q_{j}}[logq(z_{j})]$$
Also we have the chain rule 
$$p(z_{1:m},x_{1:n})=p(x_{1:n}) \prod_{j=1}^{m}p(z_{j}|z_{1:(j-1)},x_{1:n})$$
\subsubsection{Mean Field Method}
Note that the order of j is irrelevant.Based on this theory, we can rewrite the lower bound as:
\begin{align*}
\mathcal{L} &= \sum_{j=1}^{m} E_{q}[logp(z_{j}|z_{1:(j-1)},x_{1:n})]-E_{q_{j}}[logq(z_{j})] 
\end{align*}
Consider the variable $z_{j}$ comes last: 
$$\mathcal{L} =logp(x_{1:n})+E_{q}[logp(z_{j}|z_{-j},x)]-E_{q_{j}}[logq(z_{j})]$$
And we can remove the first term because it's irrevalte to $q(z_{j})$,the $\mathcal{L}$ can be written as

\begin{align*}
argmax_{q_{j}} \mathcal{L} &=E_{q}[logp(z_{j}|z_{-j},x)]-E_{q_{j}}[logq(z_{j})] \\
&= \int q(z_{j})E_{-j}[logp(z_{j}|z_{-j},x)]dz_{j}-\int q(z_{j})logq(z_{j})dz_{j}
\end{align*}

\subsection{Coordinate Ascent Variational Inference}
From here we can use Lagrange multipliers. Let's treat $q(z_{j})$ as $f(x)$.
For simplicity, I convert the formula into this:
$$\frac{d \mathcal{L}}{dq(z_{j})}=\frac{d[\int Kf(x)dx- \int f(x)logf(x)dx]}{d[f(x)]}=0$$
this is equal to:
$$\frac{d[\int Kf(x)dx- \int f(x)logf(x)dx]}{dx} \times \frac{•}{•}{dx}{d[f(x)]}=0$$
$$ [ K f(x)-f(x)logf(x)] \times \frac{1}{f'(x)}=0$$
which is :
$$Kf'(x)-[f'(x)logf(x)+f(x) \frac{1}{f(x)}f'(x)]  =0$$
and:
$$K-logf(x)-1=0$$
which means the argmax of ELBO can be find at: 
$$E_{-j}[logp(z_{j}|z_{-j},x)]-logq(z_{j})-1=0$$
$$log \frac{ e^{E_{-j}[logp(z_{j}|z_{-j},x)]}}{q(z_{j})}= log_{e}e $$
Or you can simply view it as $y-x-1=0$.
This lead to the conclusion:
$$q^{*}(z_{j}) \propto exp \left\{ E_{-j}[logp(z_{j}|z_{-j},x)] \right\}$$
since $p(z_{j}|z_{-j},x)=\frac{p(z_{j},z_{-j},x)}{p(z_{-j},x)} $, and $p(z_{-j})$ does not depend on $z_{j}$ we can equivalenty write:
$$q^{*}(z_{j}) \propto exp \left\{ E_{-j}[logp(z_{j},z_{-j},x)] \right\}$$
Note that there is generally no guarantee of convexity of ELBO, this coordinate ascent procedure converges to a local maximum.

Finally, We can find this method is closely related to Gibbs sampling. Actually Gibbs sampling is a very classical approximate inference method. In variational inference,
we take the expected log and set each variable’s variational factor iteratively.







\end{document}


























